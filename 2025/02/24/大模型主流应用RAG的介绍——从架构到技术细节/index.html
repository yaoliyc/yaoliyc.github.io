

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="屹">
  <meta name="keywords" content="">
  
    <meta name="description" content="大模型主流应用RAG的介绍——从架构到技术细节 Excerpt大语言模型（LLM）在实际应用中存在的问题；什么是RAG——检索增强生成；RAG架构解析；RAG技术架构的细节展示。    本文主要内容：  大语言模型（LLM）在实际应用中存在的问题；  什么是RAG——检索增强生成；  RAG架构解析  RAG技术架构的细节展示    写在前面如果你问我现在基于LLM（大语言模型，本文有时候也会将该">
<meta property="og:type" content="article">
<meta property="og:title" content="大模型主流应用RAG的介绍——从架构到技术细节">
<meta property="og:url" content="http://yaoliyc.github.io/2025/02/24/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B8%BB%E6%B5%81%E5%BA%94%E7%94%A8RAG%E7%9A%84%E4%BB%8B%E7%BB%8D%E2%80%94%E2%80%94%E4%BB%8E%E6%9E%B6%E6%9E%84%E5%88%B0%E6%8A%80%E6%9C%AF%E7%BB%86%E8%8A%82/index.html">
<meta property="og:site_name" content="读书笔记">
<meta property="og:description" content="大模型主流应用RAG的介绍——从架构到技术细节 Excerpt大语言模型（LLM）在实际应用中存在的问题；什么是RAG——检索增强生成；RAG架构解析；RAG技术架构的细节展示。    本文主要内容：  大语言模型（LLM）在实际应用中存在的问题；  什么是RAG——检索增强生成；  RAG架构解析  RAG技术架构的细节展示    写在前面如果你问我现在基于LLM（大语言模型，本文有时候也会将该">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://yaoliyc.github.io/2025/02/24/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B8%BB%E6%B5%81%E5%BA%94%E7%94%A8RAG%E7%9A%84%E4%BB%8B%E7%BB%8D%E2%80%94%E2%80%94%E4%BB%8E%E6%9E%B6%E6%9E%84%E5%88%B0%E6%8A%80%E6%9C%AF%E7%BB%86%E8%8A%82/640.webp">
<meta property="og:image" content="http://yaoliyc.github.io/2025/02/24/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B8%BB%E6%B5%81%E5%BA%94%E7%94%A8RAG%E7%9A%84%E4%BB%8B%E7%BB%8D%E2%80%94%E2%80%94%E4%BB%8E%E6%9E%B6%E6%9E%84%E5%88%B0%E6%8A%80%E6%9C%AF%E7%BB%86%E8%8A%82/640.1.webp">
<meta property="article:published_time" content="2025-02-24T00:34:56.000Z">
<meta property="article:modified_time" content="2025-03-02T08:58:31.680Z">
<meta property="article:author" content="屹">
<meta property="article:tag" content="RAG">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://yaoliyc.github.io/2025/02/24/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B8%BB%E6%B5%81%E5%BA%94%E7%94%A8RAG%E7%9A%84%E4%BB%8B%E7%BB%8D%E2%80%94%E2%80%94%E4%BB%8E%E6%9E%B6%E6%9E%84%E5%88%B0%E6%8A%80%E6%9C%AF%E7%BB%86%E8%8A%82/640.webp">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>大模型主流应用RAG的介绍——从架构到技术细节 - 读书笔记</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"yaoliyc.github.io","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":false,"baidu":null,"google":{"measurement_id":"G-PT3RGBW5EN"},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  

  
    <!-- Google tag (gtag.js) -->
    <script async>
      if (!Fluid.ctx.dnt) {
        Fluid.utils.createScript("https://www.googletagmanager.com/gtag/js?id=G-PT3RGBW5EN", function() {
          window.dataLayer = window.dataLayer || [];
          function gtag() {
            dataLayer.push(arguments);
          }
          gtag('js', new Date());
          gtag('config', 'G-PT3RGBW5EN');
        });
      }
    </script>
  

  

  

  



  
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>读书笔记 </strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="大模型主流应用RAG的介绍——从架构到技术细节"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-02-24 08:34" pubdate>
          2025年2月24日 早上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          4.6k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          39 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">大模型主流应用RAG的介绍——从架构到技术细节</h1>
            
            
              <div class="markdown-body">
                
                <h1 id="大模型主流应用RAG的介绍——从架构到技术细节"><a href="#大模型主流应用RAG的介绍——从架构到技术细节" class="headerlink" title="大模型主流应用RAG的介绍——从架构到技术细节"></a>大模型主流应用RAG的介绍——从架构到技术细节</h1><blockquote>
<h2 id="Excerpt"><a href="#Excerpt" class="headerlink" title="Excerpt"></a>Excerpt</h2><p>大语言模型（LLM）在实际应用中存在的问题；什么是RAG——检索增强生成；RAG架构解析；RAG技术架构的细节展示。</p>
</blockquote>
<hr>
<blockquote>
<p>本文主要内容：</p>
<ul>
<li><p>大语言模型（LLM）在实际应用中存在的问题；</p>
</li>
<li><p>什么是RAG——检索增强生成；</p>
</li>
<li><p>RAG架构解析</p>
</li>
<li><p>RAG技术架构的细节展示</p>
</li>
</ul>
</blockquote>
<h3 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h3><p>如果你问我现在基于LLM（大语言模型，本文有时候也会将该名词描述为“大模型”）最火热的应用是什么，那我必须主推检索增强生成（<strong>RAG，Retrieval Augmented Generation</strong>）。RAG最初是为了解决LLM的各类问题的（后面会提到）产生的，但后面大家发现在现阶段的很多企业痛点上，使用RAG好像是更好的解决方案。就像我之前的文章<a target="_blank" rel="noopener" href="http://mp.weixin.qq.com/s?__biz=MzIyOTA5NTM1OA==&mid=2247484135&idx=1&sn=82e93f5c69f11b60f3e8bb1afb5d5107&chksm=e846a036df31292022d31320db7b50b381519d24d405489be821257df7deb9f95630f090f590&scene=21#wechat_redirect">《关于LLM的一些声音总结》</a>提到的一样，企业发现其实自己要的好像是一个更好的搜索，根本不关心是不是大模型。于是，RAG被越来越多提到，包括开源的ChatPDF，也是RAG的一个经典应用。</p>
<p>但是我相信很多去实践RAG的人已经发现了一个情况，就是RAG入门很简单，但要真正达到企业应用的要求很难。而且RAG组成中的各类组件、流程和AI技术都过于复杂，很多人不知道如何去下手优化。</p>
<p>所以本文我们就来聊聊RAG，以及关于RAG优化的一些看法。</p>
<h3 id="LLM的问题"><a href="#LLM的问题" class="headerlink" title="LLM的问题"></a>LLM的问题</h3><p>首先我们还是要承认，RAG不管多厉害，它还是基于LLM的，脱离了LLM，RAG会脱离“人味儿”。</p>
<p>但是在今年年初的那一波大模型潮里面，我们发现如果仅仅依靠LLM，会有很多限制阻碍着我们前进，以下三点是最主要的：</p>
<ul>
<li><p><strong>幻觉问题</strong>：大模型的底层原理是基于概率，所以它有时候会一本正经胡说八道，比如我们问大模型的Chat（问答系统），“XXX博物院下周一开门吗？”我相信这样的问题你不能联系问，因为大模型会有一定的几率告诉你开门。而如果游客真的在下周一去了XXX博物院，那估计就要失望了，如果这个Chat还是博物院官方提供的，那事情最终会演变成一通12345的投诉电话。所以在很多需要非常精确的场景，仅仅依赖GPT的这种生成式回答是很不严谨的，而且看很难消除——目前常见的解决方案是前置一个BERT，或者预置大量prompt做优化。</p>
</li>
<li><p><strong>新鲜度问题</strong>：规模越大（参数越多、tokens越多），大模型训练的成本越高。类似OpenAI的ChatGPT3.5，目前的数据新鲜度依然保留在2021年，对于之后的事情就不知道了。而且对于一些高时效性的事情，大模型更加无能为力，比如帮我看看今天晚上有什么电影值得去看？这种任务是需要去淘票票、猫眼等网站先去获取最新电影信息的，大模型本身无法完成这个任务。</p>
</li>
<li><p><strong>数据安全</strong>：OpenAI已经遭到过几次隐私数据的投诉，而对于企业来说，如果把自己的经营数据、合同文件等机密文件和数据上传到互联网上的大模型，那想想都可怕。如果企业人员想提一个类似这样的问题：“帮我看看3月份XX部门的销售环比数据与哪些兄弟部门的增长是密切相关的？”，这需要打穿企业内部的很多数据。既要保证安全，又要借助AI能力，那么最好的方式就是把数据全部放在本地，企业数据的业务计算全部在本地完成。而在线的大模型仅仅完成一个归纳的功能，甚至，LLM都可以完全本地化部署。</p>
</li>
</ul>
<p>其实问题还有很多，包括tokens的限制，虽然这个长期来看不是问题，各LLM供应商的tokens数量限制肯定会越来越大。但是，费用也许就是另外一个需要考虑的问题了。</p>
<p>在解决这些问题的方法上，目前RAG走得比较前面。有些朋友应该已经了解过LangChain，它是架在LLM之上的一个应用框架，帮助人们快速开发基于LLM的应用。它的很多功能其实也属于RAG范畴。</p>
<h3 id="什么是RAG"><a href="#什么是RAG" class="headerlink" title="什么是RAG"></a>什么是RAG</h3><p>为RAG铺垫了这么多，下面我们来看看什么是RAG。</p>
<p>从开头的介绍里面大家已经可以看到，RAG——Retrieval Augmented Generation，检索增强生成。它的主要作用是生成（最终的答案），但是它先做了对现有文档的检索，而不是任由LLM来发挥。下面我提供一个浅显的例子来说明一下RAG：</p>
<p>假设一个工程师需要从厚厚的《业务操作手册》中找到相关的业务知识来帮助他完成工作，那么他有三种方式可以使用：</p>
<ul>
<li><p><strong>最原始</strong>：他可以去翻阅这么厚厚的《业务操作手册》，或者用去查询这么《业务操作手册》的电子版，然后认真阅读掌握操作方法。当然，如果他碰到的业务知识比较复杂，他就需要自己去综合这本书上面的多个章节的内容，并融会贯通；</p>
</li>
<li><p><strong>借助问答机器人</strong>：他也可以直接去咨询问答机器人（chatbot），机器人也会把相应的知识吐给你。但是它可能有两个麻烦，一是它类似于FAQ，一问一答，还是需要自己去组合所有的回答内容；二是这个机器人需要前期大量的预训练知识库，需要专业的工程师去一条一条（其实是一个个知识条目，包括答案和多个相似问法），工作量极大，不太适合大面积推广使用；</p>
</li>
<li><p><strong>RAG</strong>：RAG的操作方式就是我们可以直接把这个《业务操作手册》的电子版上传到系统，系统在几分钟之内就可以把这篇“巨著”变成索引，供刚才那位工程师咨询。而且RAG给的答案会去综合正本手册的多个相关知识点，并用“专家”一样的口吻来给你答案：“要解决这个问题，你需要先解决两个前提，有三种方法来解决。下面我们一步步来看怎么做….”。</p>
</li>
</ul>
<p>好了，我们现在知道RAG是个什么玩意儿了。你有没有发现，它其实会把之前的FAQ问答给取代掉，但是它能做的远远不止这些，它还是很多应用的中间件。我们自己的一个面向文博场馆的产品就是基于RAG的，大概占了整个系统的1&#x2F;3比重。另外，RAG不仅仅面向文本，它还可以面向语音、视频和图像等多模态场景，只要可以embedding的内容就可以，当然这些我们这里就不多介绍了。</p>
<h3 id="RAG架构"><a href="#RAG架构" class="headerlink" title="RAG架构"></a>RAG架构</h3><p>下面我们来了解一下RAG，它有非常多的组件，但是我们可以化繁为简。我喜欢把RAG——Retrieval <strong>Augmented</strong> Generation理解为Retrieval <strong>And</strong> Generation，也就是<strong>检索与生成</strong>，在加上一个数据向量和索引的工作，我们对RAG就可以总概方式地理解为“<strong>索引、检索和生成</strong>”。</p>
<p>以下就是RAG的主要组成，依次是数据提取——embedding（向量化）——创建索引——检索——自动排序（Rerank）——LLM归纳生成。当然这里少了使用环节，我们暂时先忽略用户提问的环节。</p>
<p><img src="/2025/02/24/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B8%BB%E6%B5%81%E5%BA%94%E7%94%A8RAG%E7%9A%84%E4%BB%8B%E7%BB%8D%E2%80%94%E2%80%94%E4%BB%8E%E6%9E%B6%E6%9E%84%E5%88%B0%E6%8A%80%E6%9C%AF%E7%BB%86%E8%8A%82/640.webp" srcset="/img/loading.gif" lazyload alt="图片"></p>
<p>RAG的主要流程</p>
<h3 id="RAG技术细节概览"><a href="#RAG技术细节概览" class="headerlink" title="RAG技术细节概览"></a>RAG技术细节概览</h3><p>在技术细节上，我们还可以分成更细的组成。</p>
<p><img src="/2025/02/24/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B8%BB%E6%B5%81%E5%BA%94%E7%94%A8RAG%E7%9A%84%E4%BB%8B%E7%BB%8D%E2%80%94%E2%80%94%E4%BB%8E%E6%9E%B6%E6%9E%84%E5%88%B0%E6%8A%80%E6%9C%AF%E7%BB%86%E8%8A%82/640.1.webp" srcset="/img/loading.gif" lazyload alt="图片"></p>
<p>RAG的一些技术细节  </p>
<h4 id="一、数据索引"><a href="#一、数据索引" class="headerlink" title="一、数据索引"></a>一、数据索引</h4><ul>
<li><p><strong>数据提取</strong></p>
</li>
<li><p>数据清洗：包括数据Loader，提取PDF、word、markdown以及数据库和API等；</p>
</li>
<li><p>数据处理：包括数据格式处理，不可识别内容的剔除，压缩和格式化等；</p>
</li>
<li><p>元数据提取：提取文件名、时间、章节title、图片alt等信息，非常关键。  </p>
</li>
<li><p><strong>分块</strong>（Chunking）</p>
</li>
<li><p><strong>固定大小的分块方式</strong>：一般是256&#x2F;512个tokens，取决于embedding模型的情况。但是这种方式的弊端是会损失很多语义，比如“我们今天晚上应该去吃个大餐庆祝一下”，很有可能就会被分在两个chunk里面——“我们今天晚上应该”、“去吃个大餐庆祝一下”。这样对于检索是非常不友好的，解决方法是增加冗余量，比如512tokens的，实际保存480tokens，一头一尾去保存相邻的chunk头尾的tokens内容；</p>
</li>
<li><p><strong>基于意图的分块方式</strong>：</p>
</li>
<li><p>句分割：最简单的是通过句号和换行来做切分。当然也有通过专业的意图包来切分的，常用的意图包有基于NLP的NLTK和spaCy；</p>
</li>
<li><p>递归分割：通过分而治之的思想，用递归切分到最小单元的一种方式；</p>
</li>
<li><p>特殊分割：还有很多不常见的，用于特殊场景，这里就不提了。</p>
</li>
<li><p>固定大小的分块方式：一般是256&#x2F;512个tokens，取决于embedding模型的情况。但是这种方式的弊端是会损失很多语义，比如“我们今天晚上应该去吃个大餐庆祝一下”，很有可能就会被分在两个chunk里面——“我们今天晚上应该”、“去吃个大餐庆祝一下”。这样对于检索是非常不友好的，解决方法是增加冗余量，比如512tokens的，实际保存480tokens，一头一尾去保存相邻的chunk头尾的tokens内容；</p>
</li>
<li><p><strong>影响分块策略的因素</strong>：</p>
</li>
<li><p>取决于你的索引类型，包括文本类型和长度，文章和微博推文的分块方式就会很不同；</p>
</li>
<li><p>取决于你的模型类型：你使用什么LLM也会有不同，因为ChatGLM、ChatGPT和Claude.ai等的tokens限制长度不一样，会影响你分块的尺寸；</p>
</li>
<li><p>取决于问答的文本的长度和复杂度：最好问答的文本长度和你分块的尺寸差不多，这样会对检索效率更友好；</p>
</li>
<li><p>应用类型：你的RAG的应用是检索、问答和摘要等，都会对分块策略有不同的影响。</p>
</li>
<li><p><strong>向量化</strong>（embedding）：这是将文本、图像、音频和视频等转化为向量矩阵的过程，也就是变成计算机可以理解的格式，embedding模型的好坏会直接影响到后面检索的质量，特别是相关度。关于embedding大家可以看我之前的一篇文章《<a target="_blank" rel="noopener" href="http://mp.weixin.qq.com/s?__biz=MzIyOTA5NTM1OA==&mid=2247484246&idx=1&sn=c0b4f64e829bc336d516b4c79bbda19c&chksm=e846a187df31289169516fdf09ebfc3235dfcc11f467cf9541b022b2f3381a5c464db37f4da6&scene=21#wechat_redirect">大模型应用中大部分人真正需要去关心的核心——Embedding</a>》，一般我们现在可以选择的embedding模型有这些：</p>
</li>
<li><p><strong>BGE</strong>：这是国人开发的中文embedding模型，在HuggingFace的MTEB（海量文本Embedding基准）上排名前2，实力强劲；</p>
</li>
<li><p><strong>M3E</strong>：也是国人开发的中文embedding模型，我们之前用的就是这个模型，总体来说也算可以，这个还看大家的使用场景，也许你的场景会比我们更加适用；</p>
</li>
<li><p><strong>通义千问的embedding模型</strong>：因为是1500+维的模型，所以我们在国庆节后准备用用看；</p>
</li>
<li><p><strong>Text-embedding-ada-002</strong>：这是OpenAI的embedding模型，1536维，我感觉上应该是目前最好的模型，但是它在MTEB上排名好像只有第六，但是国内应该也不太能用，所以我们就放弃了；</p>
</li>
<li><p><strong>自己训练embedding模型</strong>：<strong>这是最酷的了，我过几天会专门写一篇如何训练embedding模型的文章，没有关注我的可以先关注，哈</strong>。当然，训练是基于一个既有embedding模型的，一般我们有希望让它在原来的基础上提升3%-10%的性能。</p>
</li>
</ul>
<h4 id="二、检索环节（Retriever）"><a href="#二、检索环节（Retriever）" class="headerlink" title="二、检索环节（Retriever）"></a>二、检索环节（Retriever）</h4><p>检索环节技术含量依然很高，而且对于我们目前来说，还有一两项工作正在进行中。</p>
<p>检索优化一般分为下面五部分工作：</p>
<ul>
<li><p><strong>元数据过滤</strong>：当我们把索引分成许多chunks的时候，检索效率会成为问题。这时候，如果可以通过元数据先进行过滤，就会大大提升效率和相关度。比如，我们问“帮我整理一下XX部门今年5月份的所有合同中，包含XX设备采购的合同有哪些？”。这时候，如果有元数据，我们就可以去搜索“<strong>XX部门+2023年5月</strong>”的相关数据，检索量一下子就可能变成了全局的万分之一；</p>
</li>
<li><p><strong>图关系检索</strong>：如果可以将很多实体变成node，把它们之间的关系变成relation，就可以利用知识之间的关系做更准确的回答。特别是针对一些多跳问题，利用图数据索引会让检索的相关度变得更高；</p>
</li>
<li><p><strong>检索技术</strong>：前面说的是一些前置的预处理的方法，检索的主要方式还是这几种：</p>
</li>
<li><p><strong>相似度检索</strong>：前面我已经写过那篇文章《<a target="_blank" rel="noopener" href="http://mp.weixin.qq.com/s?__biz=MzIyOTA5NTM1OA==&mid=2247484246&idx=1&sn=c0b4f64e829bc336d516b4c79bbda19c&chksm=e846a187df31289169516fdf09ebfc3235dfcc11f467cf9541b022b2f3381a5c464db37f4da6&scene=21#wechat_redirect">大模型应用中大部分人真正需要去关心的核心——Embedding</a>》中有提到六种相似度算法，包括欧氏距离、曼哈顿距离、余弦等，后面我还会再专门写一篇这方面的文章，可以关注我，yeah；</p>
</li>
<li><p><strong>关键词检索</strong>：这是很传统的检索方式，但是有时候也很重要。刚才我们说的元数据过滤是一种，还有一种就是先把chunk做摘要，再通过关键词检索找到可能相关的chunk，增加检索效率。据说Claude.ai也是这么做的；</p>
</li>
<li><p><strong>SQL检索</strong>：这就更加传统了，但是对于一些本地化的企业应用来说，SQL查询是必不可少的一步，比如我前面提到的销售数据，就需要先做SQL检索。</p>
</li>
<li><p>其他：检索技术还有很多，后面用到再慢慢说吧。</p>
</li>
<li><p><strong>重排序（Rerank）</strong>：很多时候我们的检索结果并不理想，原因是chunks在系统内数量很多，我们检索的维度不一定是最优的，一次检索的结果可能就会在相关度上面没有那么理想。这时候我们需要有一些策略来对检索的结果做重排序，比如使用planB重排序，或者把组合相关度、匹配度等因素做一些重新调整，得到更符合我们业务场景的排序。因为在这一步之后，我们就会把结果送给LLM进行最终处理了，所以这一部分的结果很重要。这里面还会有一个内部的判断器来评审相关度，触发重排序。</p>
</li>
<li><p><strong>查询轮换</strong>：这是查询检索的一种方式，一般会有几种方式：</p>
</li>
<li><p><strong>子查询：</strong>可以在不同的场景中使用各种查询策略，比如可以使用LlamaIndex等框架提供的查询器，采用树查询（从叶子结点，一步步查询，合并），采用向量查询，或者最原始的顺序查询chunks等<strong>；</strong></p>
</li>
<li><p><strong>HyDE：</strong>这是一种抄作业的方式，生成相似的或者更标准的prompt模板<strong>。</strong></p>
</li>
<li><h4 id="三、生成（Gen）"><a href="#三、生成（Gen）" class="headerlink" title="三、生成（Gen）"></a>三、生成（Gen）</h4><p>这一步反而是我比较疏忽的，因为有大量的现成框架可以使用，而且，这一步真正发挥巨大作用的是LLM。</p>
<p>这里面我们使用的框架有Langchain和LlamaIndex，而且我们因为有之前的AI产品积累，所以还有一套完整的Java框架可以使用，所以这一块我没有太多研究。唯一非常关注的就是Prompt工程，我们团队内部，这一部分的工作是交给了原来AI产品的知识库运营团队来做的，他们原来做的更多是BERT相关的知识库预训练，应该说工作内容还是比较匹配的。</p>
<p>在Prompt里面其实还是有很多决定因素的，这和大家对所处行业的knowhow有关。比如在文旅行业，你要知道游客或者观众一般会怎么提问，真正需要得到的是什么内容。这些我就不多说了，各行业不太一样，但是大家可以翻看我之前写的关于prompt的文章<a target="_blank" rel="noopener" href="http://mp.weixin.qq.com/s?__biz=MzIyOTA5NTM1OA==&mid=2247483977&idx=1&sn=b6eaf2e7b3ae2a6446ad3d4a5ea2e7ab&chksm=e846a098df31298e3f827106128532b2034d3c6ab2442a6ad2eda3f2cfc399103c2a2ae278e5&scene=21#wechat_redirect">《一文讲清楚实用Prompt工程》</a>、<a target="_blank" rel="noopener" href="http://mp.weixin.qq.com/s?__biz=MzIyOTA5NTM1OA==&mid=2247484018&idx=1&sn=8d517b484f6752acf835a8e5bbd0703d&chksm=e846a0a3df3129b5f95912f62a46d472cf38a958985f364d62ed3816888775bfce2d6f0442b5&scene=21#wechat_redirect">《高级prompt工程讲解》</a>。</p>
<p>其他的关于ReAct等内容，本文就先不涉及了，后面再专门写一篇文章。</p>
</li>
</ul>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>LLM这一波，催生的技术真的很多，每一个环节，要真正做好，符合企业应用，都可以让我们研究好长一段时间，并需要不断去实践，才能打磨出精品。但是如果打磨出精品，你肯定可以摘取新技术带来的果实。技术人员在很多公司地位不高？嗯，大模型这一波，我已经感觉到变化的发生了——了解大模型和不了解大模型的人，在AI的业务上的理解上，有某些角度来看，他们的区别真的就现代人类和史前人类一样巨大。</p>
<p>本文可能还是属于大模型应用——RAG的一个大纲式的技术普及文章，这里面的很多技术细节，我会在后面一篇篇专门来写，<strong>欢迎大家关注“土猛的员外”</strong>。</p>
<p><strong>近期文章：</strong>  </p>
<p><a target="_blank" rel="noopener" href="http://mp.weixin.qq.com/s?__biz=MzIyOTA5NTM1OA==&mid=2247484262&idx=1&sn=430270e10268c4b97c3b5d983fdfb75b&chksm=e846a1b7df3128a139091d31e4793e2fdcb391da2e866cd914d0f0ecf38ce2d9f285d78c9a03&scene=21#wechat_redirect">最详细的文本分块(Chunking)方法——可以直接影响基于LLM应用效果</a>  </p>
<p><a target="_blank" rel="noopener" href="http://mp.weixin.qq.com/s?__biz=MzIyOTA5NTM1OA==&mid=2247484246&idx=1&sn=c0b4f64e829bc336d516b4c79bbda19c&chksm=e846a187df31289169516fdf09ebfc3235dfcc11f467cf9541b022b2f3381a5c464db37f4da6&scene=21#wechat_redirect">大模型应用中大部分人真正需要去关心的核心——Embedding</a>  </p>
<p><a target="_blank" rel="noopener" href="http://mp.weixin.qq.com/s?__biz=MzIyOTA5NTM1OA==&mid=2247484222&idx=1&sn=38e01375f39c9a1cba35a522ebda0158&chksm=e846a1efdf3128f91cc67f1db97f13f391490c96230a916bc9a33c2a8463f91bc5a615f72537&scene=21#wechat_redirect">适合你找个时间好好消化的文章，大模型核心技术——Transformer架构</a>  </p>
<p><a target="_blank" rel="noopener" href="http://mp.weixin.qq.com/s?__biz=MzIyOTA5NTM1OA==&mid=2247484189&idx=1&sn=17bb6c4eb4d35d525796cbd5dc503bc7&chksm=e846a1ccdf3128daedb62b0738c3f40322fa5cb5898e7fba99176de416c65eb92faea7e160ae&scene=21#wechat_redirect">不开玩笑，1800亿参数的Falcon大模型可以在家用电脑上部署了</a></p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/RAG/" class="category-chain-item">RAG</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/RAG/" class="print-no-link">#RAG</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>大模型主流应用RAG的介绍——从架构到技术细节</div>
      <div>http://yaoliyc.github.io/2025/02/24/大模型主流应用RAG的介绍——从架构到技术细节/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>屹</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年2月24日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/02/24/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E4%B8%AD%E5%A4%A7%E9%83%A8%E5%88%86%E4%BA%BA%E7%9C%9F%E6%AD%A3%E9%9C%80%E8%A6%81%E5%8E%BB%E5%85%B3%E5%BF%83%E7%9A%84%E6%A0%B8%E5%BF%83%E2%80%94%E2%80%94Embedding/" title="大模型应用中大部分人真正需要去关心的核心——Embedding">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">大模型应用中大部分人真正需要去关心的核心——Embedding</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/02/22/%E5%9F%BA%E4%BA%8E-AnythingLLM-%E5%8F%8A-Ollama-%E6%9E%84%E5%BB%BA%E6%9C%AC%E5%9C%B0%E7%9F%A5%E8%AF%86%E5%BA%93/" title="基于 AnythingLLM 及 Ollama 构建本地知识库">
                        <span class="hidden-mobile">基于 AnythingLLM 及 Ollama 构建本地知识库</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="twikoo"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#comments', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/twikoo/1.6.8/twikoo.all.min.js', function() {
        var options = Object.assign(
          {"envId":"https://sweet-faun-1bb615.netlify.app/.netlify/functions/twikoo","region":"ap-shanghai","path":"window.location.pathname"},
          {
            el: '#twikoo',
            path: 'window.location.pathname',
            onCommentLoaded: function() {
              Fluid.utils.listenDOMLoaded(function() {
                var imgSelector = '#twikoo .tk-content img:not(.tk-owo-emotion)';
                Fluid.plugins.imageCaption(imgSelector);
                Fluid.plugins.fancyBox(imgSelector);
              });
            }
          }
        )
        twikoo.init(options)
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量 
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数 
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    

  

</div>

  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
